{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn import metrics\n",
    "from notebooks.sentiment.util import build_pipeline, print_eval, preprocess_tweets\n",
    "from sentiment.new_data import InterTASSAugmented\n",
    "from sentiment.tass import InterTASSReader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gosen/.local/share/virtualenvs/PLN-2019-N1iczKTA/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/gosen/.local/share/virtualenvs/PLN-2019-N1iczKTA/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('feats', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('vect', TfidfVectorizer(analyzer='char_wb', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.95, max_features=None, min_df=5,\n",
       "        ngr...penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reader = InterTASSAugmented(\"2augmented_data_heu\", ratio=0.9)  # Class to use augmented data\n",
    "# X_train, y_train = reader.Xy()\n",
    "train = \"intertass-ES-train-tagged.xml\"\n",
    "train = InterTASSReader(train)\n",
    "X_train, y_train = list(train.X()), list(train.y())\n",
    "\n",
    "\n",
    "corpus = \"intertass-ES-development-tagged.xml\"\n",
    "dev = InterTASSReader(corpus)\n",
    "X_dev, y_dev = list(dev.X()), list(dev.y())\n",
    "\n",
    "pipeline = build_pipeline()\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\t0.56\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           n       0.58      0.77      0.66       219\n",
      "           p       0.20      0.07      0.11        69\n",
      "        none       0.24      0.10      0.14        62\n",
      "         neu       0.61      0.65      0.63       156\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       506\n",
      "   macro avg       0.41      0.40      0.38       506\n",
      "weighted avg       0.50      0.56      0.51       506\n",
      "\n",
      "[[168  11   9  31]\n",
      " [ 40   5   4  20]\n",
      " [ 37   5   6  14]\n",
      " [ 44   4   6 102]]\n"
     ]
    }
   ],
   "source": [
    "print_eval(pipeline, X_dev, y_dev, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('feats', FeatureUnion(n_jobs=None,\n",
       "         transformer_list=[('vect', TfidfVectorizer(analyzer='char_wb', binary=True, decode_error='strict',\n",
       "          dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=0.95, max_features=None, min_df=5,\n",
       "          ngram_range=(1, 6), norm='l2',\n",
       "          preprocessor=<...      tokenizer=<function word_tokenize at 0x7fb6a6d0ed90>, use_idf=True,\n",
       "          vocabulary=None))],\n",
       "         transformer_weights=None)),\n",
       " ('clf', LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "            solver='warn', tol=0.0001, verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspeccion de modelos:\n",
    "\n",
    "**El modelo a analizar fue entrenado usando regresion logistica y la union\n",
    "de dos clasificadores, uno que usó \"word\" como _analyzer_ y el otro uso \"char\".\n",
    "Veamos cuales son los 10 features con mas peso positivo y mas peso negativo para cada clase:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N:\n",
      "\t! ! !   A guapa rt s! rac eli 1 ([-0.83341789 -0.71006416 -0.69528888 -0.39891781 -0.39116628 -0.35919981\n",
      " -0.35230278 -0.34752441 -0.34490157 -0.34457249])\n",
      "\todi me  no od  me  triste  n  no no   no  ([0.45335267 0.49109051 0.49409086 0.50665793 0.54603738 0.59735805\n",
      " 0.66252618 0.77205035 0.90183338 0.91858853])\n",
      "NEU:\n",
      "\tj ap ño am ?  y   L L uc ol ([-0.54014245 -0.49413524 -0.46230685 -0.44611203 -0.42453641 -0.42350058\n",
      " -0.42126836 -0.41817165 -0.40529656 -0.40098482])\n",
      "\tser cereal cereal hacen tio Estoy nerviosa Lo hecho , dicho  lo  plan bonito serio viejas ([0.59900622 0.59900622 0.61462497 0.63610812 0.64944056 0.65468827\n",
      " 0.6589533  0.73702051 0.95766574 1.04788207])\n",
      "NONE:\n",
      "\t mu mu  m  me á  no   me   muy  muy  uy  ([-0.63417026 -0.62699317 -0.568688   -0.56681019 -0.54344921 -0.51474655\n",
      " -0.50087856 -0.48762919 -0.48762919 -0.48095096])\n",
      "\tct 0  stra si Votado abstracto juntos ?  ? ? ([0.5468255  0.55525434 0.57163574 0.58832631 0.78823053 0.91056084\n",
      " 0.92608845 0.98436306 1.02487429 1.14208342])\n",
      "P:\n",
      "\t no  no   no  pe No  n triste tal   No  di ([-0.68553479 -0.60455401 -0.52820494 -0.49297995 -0.45532937 -0.43137937\n",
      " -0.40544318 -0.38079914 -0.3806207  -0.37914653])\n",
      "\traci rac  g # gra guapa uen !  ! ! ([0.54496372 0.54691464 0.58815541 0.59111556 0.59274551 0.66817965\n",
      " 0.68702551 0.9177862  0.94776145 1.11551817])\n"
     ]
    }
   ],
   "source": [
    "vect = pipeline.named_steps['feats']\n",
    "clf = pipeline.named_steps['clf']\n",
    "from sentiment.analysis import print_maxent_features\n",
    "print_maxent_features(vect, clf, n=10, prettify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**En aspectos generales creo que la mayoria de los features tienen sentido, pero dado que se usan ambos analizadores y no solo el que toma palabras vemos que aparecen varios conjuntos de letras que no significan una palabra en sí, como \"rt\", \"mu\", \"me\", \"uy\", \"ct\" etc. entonces, esto complica el analisis.\n",
    "Pero si vamos a los casos mas claros, por ejemplo para la clase los positivos se ve claramente como la palabra \"no\" y la palabra \"triste\" son los features que mas desfavorecen la clase, lo que es correcto.\n",
    "Después por ejemplo, en los neutros, vemos palabras como \"cereal\", \"ser\", \"hecho\", \"nerviosa\" que en si estas palabras, no tienen una carga de sentimiento por si mismas, si no que dependen del contexto en el cual son utilizadas.\n",
    "Como feature, engañoso o incorrecto, pueden ser los que son letras que no se sabe a que palabra refieren o por que tienen tanto peso, como que \"eli\" tiene peso negativo en la clase negativa, pero a priori no se sabe a que palabra corresponde \"eli\".\n",
    "Otro punto importante que vale la pena observar, es el gran peso que tienen los signos de exclamacion en la clase positiva, o los signos de interrogacion en la clase \"NONE\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
